{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bb43e43",
   "metadata": {},
   "source": [
    "### Test URLS\n",
    "https://sunstonepartners.com \\\n",
    "https://www.appliedlearning.com \\\n",
    "https://www.forsalebyowner.com login \\\n",
    "https://ecmins.com/ \\\n",
    "http://www.iconnect-corp.com \\\n",
    "https://cessco.ca/ ROBOT \\\n",
    "http://www.ticss.net \\\n",
    "https://www.tyremarket.com/Car-Tyres \\\n",
    "https://www.dentalxchange.com/ \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e2db29",
   "metadata": {},
   "source": [
    "**TODOList**:\n",
    "\n",
    "-Improve URL relevance check (exclude /#, /login, /sign-up)\n",
    "\n",
    "-Never return empty nav object, instead string. If after try bs4 and sel, fails\n",
    "\n",
    "-script defers to selenium if bs4 does nav_scrape but nav still empty (ex: https://www.forsalebyowner.com)\n",
    "\n",
    "-File output with all columns\n",
    "\n",
    "-Enhance href relevance function (both contain base_url)\n",
    "\n",
    "-Four columns of information for each website\n",
    "\n",
    "-Improve speed of sel nav_tree recursion\n",
    "\n",
    "-retry if page_result is empty after page scrape\n",
    "\n",
    "-page scrape for bs4 (page scrape working for sel)\n",
    "\n",
    "-assess whitespace split to help headers\n",
    "\n",
    "-requests 200 requirement for first href selection\n",
    "\n",
    "-account for more options in 'assess' functions\n",
    "\n",
    "-add website_url parameter into sel_nav_scrape for consistency\n",
    "\n",
    "-FIX nav scrape for sel (nav scrape working for bs4). Specifically, first_href - not critical because very slow.\n",
    "\n",
    "> Figure out alternative when no nav (all a tags' hrefs or first relevant href?) (ex: https://www.forsalebyowner.com) \\\n",
    "> Make sure that first_href returned is a URL in both bs4_nav_scrape() and sel_nav_scrape() \\\n",
    "\n",
    "-first text on page (home/about)\n",
    "\n",
    "-mistral given as many pages as possible (via nav) -> really slow (~15 mins) save until next step\n",
    "\n",
    "-utilize irrelevant in asses_href\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4148c0c7",
   "metadata": {},
   "source": [
    "### All imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d4adafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import validators\n",
    "import time\n",
    "from openai import OpenAI\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium_stealth import stealth\n",
    "from selenium import webdriver\n",
    "import re\n",
    "import urllib.parse as up\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import asyncio\n",
    "from urllib.parse import urlparse\n",
    "import aiohttp\n",
    "import nest_asyncio\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbaebf3",
   "metadata": {},
   "source": [
    "### Important meta tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfb13832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta_tags(url): #: str) -> dict[str,str]:\n",
    "    api_key = os.getenv('JSON_LINKS_KEY')\n",
    "\n",
    "#     url = 'https://cessco.ca/'\n",
    "    # url = 'https://www.appliedlearning.com'\n",
    "\n",
    "    params = {'url': url, 'api_key': api_key}\n",
    "    \n",
    "    # Free JsonLink limit is 30 req/minute so wait 3 seconds just in case\n",
    "    time.sleep(3)\n",
    "    \n",
    "    response = requests.get('https://jsonlink.io/api/extract', params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "#         print(data)\n",
    "        print('Title: ', data['title'], '\\nDescription: ', data['description'], '\\nDomain: ', data['domain'])\n",
    "        return {'metadata':{k: data.get(k,None) for k in ('title', 'description', 'domain')}}\n",
    "    else:\n",
    "        print(f'JSONLink Error for {url}: {response.status_code} - {response.text}')\n",
    "        return {'metadata': 'Metadata unavailable'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b4a94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_meta_tags('https://www.dentalxchange.com/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4f6fbc",
   "metadata": {},
   "source": [
    "### Handle navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa4f8eb",
   "metadata": {},
   "source": [
    "#### Get all a tags (get best nav)\n",
    "1. test if href valid url \\\n",
    "2. test if url + (optional /) + href valid url \\\n",
    "3. likely a bust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602cb6bd",
   "metadata": {},
   "source": [
    "####  Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "20acd480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sel_normalize_whitespace(text):\n",
    "    # Replace one or more whitespace characters (including spaces, tabs, and newlines) with a single space\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def sel_assess_href(base_url: str, href: str) -> str:\n",
    "    if not validators.url(href):\n",
    "        href = up.urljoin(base_url,href)\n",
    "    # Add functionality here to compare if one is contained in the other\n",
    "    return [href, 'relevant' if up.urlparse(href).netloc == up.urlparse(base_url).netloc else 'irrelevant']\n",
    "\n",
    "def sel_find_relevant_hrefs(driver):\n",
    "    atags = driver.find_elements(\"xpath\",\"//a\")\n",
    "    relevant_hrefs = []\n",
    "    for a in atags:\n",
    "        text, href = sel_normalize_whitespace(a.get_attribute('textContent')),a.get_attribute('href')\n",
    "        website_url = driver.current_url\n",
    "        assessed_href = sel_assess_href(website_url, href)\n",
    "        if assessed_href[1] == 'relevant' and (assessed_href[0] != website_url and text != 'Skip to content'):\n",
    "            relevant_hrefs += [(text, assessed_href[0])]\n",
    "    return relevant_hrefs\n",
    "\n",
    "def sel_find_first_href(home_page_url, nested_list) -> str:\n",
    "    for item in nested_list:    \n",
    "        if isinstance(item, list):\n",
    "                    # Recursively search within the list\n",
    "                    result = sel_find_first_href(home_page_url, item)\n",
    "                    if result:  # If a valid URL is found in the recursion, return it\n",
    "                        return result\n",
    "        elif isinstance(item, tuple) and len(item) == 2:\n",
    "            # If the item is a tuple with 2 elements, check the second element for a relevant URL\n",
    "            if validators.url(item[1]) and item[1] != home_page_url:\n",
    "                return item[1]  # Return the URL if it's valid\n",
    "    return 'No href found'\n",
    "\n",
    "def sel_build_tree(base_url,element):\n",
    "    # Initialize the node with tag name and text content\n",
    "    node_contents = {'text': sel_normalize_whitespace(element.get_attribute('textContent')),\n",
    "                     'href': sel_assess_href(base_url,element.get_attribute('href'))[0]} if element.tag_name == 'a' else {}\n",
    "    node = {\n",
    "        **node_contents,\n",
    "        'children': []\n",
    "    }\n",
    "\n",
    "    # Recursively build the tree for each child element\n",
    "    children = element.find_elements(By.XPATH, \"./*\")  # Only direct children\n",
    "    for child in children:\n",
    "        node['children'].append(sel_build_tree(base_url,child))\n",
    "\n",
    "    if not node['children'] or all(not obj for obj in node['children']):\n",
    "        del node['children']\n",
    "\n",
    "    return node\n",
    "\n",
    "def sel_convert_tree(root) -> list[list,int]:\n",
    "    ans, total_hrefs = [], 0\n",
    "    if 'children' not in root:\n",
    "        if 'text' in root and 'href' in root:\n",
    "            return (root['text'], root['href'], 1)\n",
    "    else:\n",
    "        for child in root['children']:\n",
    "            links = sel_convert_tree(child)\n",
    "            if links:\n",
    "                total_hrefs += links[-1]\n",
    "                ans += [links[:-1]]\n",
    "\n",
    "    return [*list(filter(None, ans)), total_hrefs]\n",
    "\n",
    "# TODO: Fix this - first_href must be a URL (even if no nav, find first relevant href but need to specify def of relevant)\n",
    "# Returns the nav tree (either advanced nested or basic list of hrefs) and first href\n",
    "def sel_nav_scrape(driver) -> list[list[tuple[str,str]],str]:\n",
    "    sel_nav_return, nav_trees = [], []\n",
    "    home_page_url = driver.current_url\n",
    "\n",
    "    # Find navs, construct trees and find max\n",
    "    navs = driver.find_elements(\"xpath\",\"//nav\")\n",
    "    for nav in navs:\n",
    "        nav_trees.append(sel_build_tree(home_page_url, nav))\n",
    "    max_nav = ({}, 0)\n",
    "    for tree in nav_trees:\n",
    "        converted = sel_convert_tree(tree)\n",
    "        if converted[-1] > max_nav[-1]:\n",
    "            max_nav = converted\n",
    "    # [:-1] to account for nested tree\n",
    "    max_nav_tree = max_nav[:-1]\n",
    "\n",
    "    # Construct return\n",
    "    if not max_nav[0]:\n",
    "    # if max_nav == ({}, 0): #or len(max_nav[-1]) < x:\n",
    "        # If no/not enough navs, find all relevant atags\n",
    "        relevant_hrefs = sel_find_relevant_hrefs(driver)  \n",
    "        sel_nav_return.append(relevant_hrefs)\n",
    "        first_href = relevant_hrefs[0][1] if relevant_hrefs else 'No href found'\n",
    "    else:\n",
    "        sel_nav_return.append(max_nav_tree)\n",
    "        first_href = sel_find_first_href(home_page_url, max_nav)\n",
    "\n",
    "    sel_nav_return.append(first_href)\n",
    "    return sel_nav_return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b15655",
   "metadata": {},
   "source": [
    "**Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805e195b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nav_driver = webdriver.Chrome()\n",
    "nav_driver.get('https://forsalebyowner.com/')\n",
    "sel_nav = sel_nav_scrape(nav_driver)\n",
    "nav_driver.close()\n",
    "sel_nav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd00960",
   "metadata": {},
   "source": [
    "#### bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e6192758",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[[('How We Help', ''),\n",
       "    [[[('Get More Customers',\n",
       "        'https://www.scorpion.co/how-we-help/get-more-customers/'),\n",
       "       [[[('Scorpion ConnectConvert visitors into customers',\n",
       "           'https://www.scorpion.co/how-we-help/scorpion-connect/')],\n",
       "         [('AI ChatBetter Client Engagement',\n",
       "           'https://www.scorpion.co/how-we-help/ai-chat/')],\n",
       "         [('Content MarketingAttract attention with quality content',\n",
       "           'https://www.scorpion.co/how-we-help/content-marketing/')],\n",
       "         [('Digital AdvertisingGet more and better leads',\n",
       "           'https://www.scorpion.co/how-we-help/digital-advertising/')],\n",
       "         [('Search Engine RankingShow up in search results',\n",
       "           'https://www.scorpion.co/how-we-help/search-engine-ranking/')],\n",
       "         [('Video MarketingTell your story through video',\n",
       "           'https://www.scorpion.co/how-we-help/video-marketing/')],\n",
       "         [('WebsiteConvert more of your traffic',\n",
       "           'https://www.scorpion.co/how-we-help/website/')]]]],\n",
       "      [('Fill Your Schedule',\n",
       "        'https://www.scorpion.co/how-we-help/fill-your-schedule/'),\n",
       "       [[[('ChatChat with customers',\n",
       "           'https://www.scorpion.co/how-we-help/chat/')],\n",
       "         [('CommunicationsConnect with customers instantly',\n",
       "           'https://www.scorpion.co/how-we-help/communications/')],\n",
       "         [('Lead ManagementManage all leads in one place',\n",
       "           'https://www.scorpion.co/how-we-help/leads-management/')],\n",
       "         [('Email MarketingBuild loyalty with email',\n",
       "           'https://www.scorpion.co/how-we-help/email-marketing/')],\n",
       "         [('Online SchedulingMake online scheduling effortless',\n",
       "           'https://www.scorpion.co/how-we-help/online-scheduling/')],\n",
       "         [('Social MediaEngage on social',\n",
       "           'https://www.scorpion.co/how-we-help/social-media/')]]]],\n",
       "      [('Build Your Brand',\n",
       "        'https://www.scorpion.co/how-we-help/build-your-brand/'),\n",
       "       [[[('Brand StrategyPosition your brand to win',\n",
       "           'https://www.scorpion.co/how-we-help/brand-strategy/')],\n",
       "         [('Creative StrategyBuild a library of custom creative assets',\n",
       "           'https://www.scorpion.co/how-we-help/creative-services/')],\n",
       "         [('Brand ResearchUnderstand your customers',\n",
       "           'https://www.scorpion.co/how-we-help/brand-research/')],\n",
       "         [('PhotographyShow clients who you are',\n",
       "           'https://www.scorpion.co/how-we-help/photography/')]]]],\n",
       "      [('Improve Your Reputation',\n",
       "        'https://www.scorpion.co/how-we-help/improve-your-reputation/'),\n",
       "       [[[('Review MonitoringStay on top of your reputation',\n",
       "           'https://www.scorpion.co/how-we-help/review-monitoring/')],\n",
       "         [('SurveysLearn with surveys',\n",
       "           'https://www.scorpion.co/how-we-help/surveys/')]]]],\n",
       "      [('Get Paid Faster',\n",
       "        'https://www.scorpion.co/how-we-help/get-paid-faster/'),\n",
       "       [[[('PaymentsShorten your collection period',\n",
       "           'https://www.scorpion.co/how-we-help/payments/')],\n",
       "         [('InvoicesFaster invoicing',\n",
       "           'https://www.scorpion.co/how-we-help/invoices/')]]]],\n",
       "      [('Track Your Results',\n",
       "        'https://www.scorpion.co/how-we-help/track-your-results/'),\n",
       "       [[[('AI InsightsPrioritize decisions with quality data',\n",
       "           'https://www.scorpion.co/how-we-help/ai-insights/')],\n",
       "         [('IntegrationsGet your tools working together',\n",
       "           'https://www.scorpion.co/how-we-help/integrations/')],\n",
       "         [('Mobile AppRun your business on the go',\n",
       "           'https://www.scorpion.co/how-we-help/mobile-app/')],\n",
       "         [('ReportingGet real-time insights',\n",
       "           'https://www.scorpion.co/how-we-help/reporting/')]]]]]]],\n",
       "   [('Who We Help', ''),\n",
       "    [[[('Home Services', 'https://www.scorpion.co/home-services/'),\n",
       "       [[[('Electricians',\n",
       "           'https://www.scorpion.co/home-services/electricians/')],\n",
       "         [('HVAC', 'https://www.scorpion.co/home-services/hvac/')],\n",
       "         [('Roofing', 'https://www.scorpion.co/home-services/roofing/')],\n",
       "         [('Restoration',\n",
       "           'https://www.scorpion.co/home-services/restoration/')],\n",
       "         [('Plumbing', 'https://www.scorpion.co/home-services/plumbing/')],\n",
       "         [('Pest Control',\n",
       "           'https://www.scorpion.co/home-services/pest-control/')]]]],\n",
       "      [('Healthcare', 'https://www.scorpion.co/healthcare/'),\n",
       "       [[[('Chiropractic',\n",
       "           'https://www.scorpion.co/healthcare/chiropractor/')],\n",
       "         [('Veterianary', 'https://www.scorpion.co/healthcare/veterinarian/')],\n",
       "         [('Pediatrics', 'https://www.scorpion.co/healthcare/pediatrics/')],\n",
       "         [('Urgent Care',\n",
       "           'https://www.scorpion.co/healthcare/urgent-care/')]]]],\n",
       "      [('Dental', 'https://www.scorpion.co/healthcare/dentists/'),\n",
       "       [[[('Dental Service Organizations',\n",
       "           'https://www.scorpion.co/healthcare/dental-service-organization/')],\n",
       "         [('Orthodontics',\n",
       "           'https://www.scorpion.co/healthcare/orthodontics/')]]]],\n",
       "      [('Law Firms', 'https://www.scorpion.co/law-firms/'),\n",
       "       [[[('Bankruptcy', 'https://www.scorpion.co/law-firms/bankruptcy/')],\n",
       "         [('Criminal Defense',\n",
       "           'https://www.scorpion.co/law-firms/criminal-defense/')],\n",
       "         [('Employment Law',\n",
       "           'https://www.scorpion.co/law-firms/employment-law/')],\n",
       "         [('Estate Planning & Probate',\n",
       "           'https://www.scorpion.co/law-firms/estate-probate/')],\n",
       "         [('Family Law', 'https://www.scorpion.co/law-firms/family-law/')],\n",
       "         [('Immigration', 'https://www.scorpion.co/law-firms/immigration/')],\n",
       "         [('Personal Injury',\n",
       "           'https://www.scorpion.co/law-firms/personal-injury/')]]]],\n",
       "      [('Multi-Location Brands',\n",
       "        'https://www.scorpion.co/multi-location-brands/'),\n",
       "       [[[('Health Systems',\n",
       "           'https://www.scorpion.co/multi-location-brands/health-systems/')],\n",
       "         [('Dental Service Organizations',\n",
       "           'https://www.scorpion.co/multi-location-brands/dental-service-organizations/')],\n",
       "         [('Multi-Location Brands',\n",
       "           'https://www.scorpion.co/multi-location-brands/brands/')]]]],\n",
       "      [('Franchise', 'https://www.scorpion.co/franchises/')]]]],\n",
       "   [('Why Scorpion', ''),\n",
       "    [[[('Who We Are', 'https://www.scorpion.co/about-us/')],\n",
       "      [('Scorpion Cares', 'https://www.scorpion.co/scorpion-cares/')],\n",
       "      [('Careers', 'https://www.scorpion.co/careers/')],\n",
       "      [('Partnerships', 'https://www.scorpion.co/partnerships/')]]]],\n",
       "   [('Resources', ''),\n",
       "    [[[('Blog', 'https://www.scorpion.co/articles/')],\n",
       "      [('Videos & Podcasts', 'https://www.scorpion.co/videos/')],\n",
       "      [('Growth Studies', 'https://www.scorpion.co/growth-studies/')],\n",
       "      [('Webinars', 'https://www.scorpion.co/webinars/')],\n",
       "      [('Scorpion News', 'https://www.scorpion.co/scorpion-news/')]]]]],\n",
       "  ('(866) 344-8852', 'tel:(866) 344-8852'),\n",
       "  ('Login', 'https://app.scorpion.co/'),\n",
       "  ('Get Started', 'https://www.scorpion.co/get-started/')],\n",
       " 'https://www.scorpion.co/how-we-help/get-more-customers/']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bs4_build_tree(base_url, element):\n",
    "    # Initialize the node with tag name and text content\n",
    "    node_text = {'text': element.get_text(strip=True)} if element.name == 'a' else {}\n",
    "    node = {\n",
    "        **node_text,\n",
    "        'children': []\n",
    "    }\n",
    "\n",
    "    # If it's an <a> tag, include the href attribute\n",
    "    if element.name == 'a':\n",
    "        node['href'] = bs4_assess_href(base_url,element.get('href'))[0]\n",
    "\n",
    "    # Recursively build the tree for each child element\n",
    "    for child in element.find_all(recursive=False):  # Only direct children\n",
    "        node['children'].append(bs4_build_tree(base_url, child))\n",
    "        \n",
    "    if not node['children'] or all(not obj for obj in node['children']):\n",
    "        del node['children']\n",
    "\n",
    "    return node\n",
    "\n",
    "def bs4_convert_tree(root):\n",
    "    ans, total_hrefs = [], 0\n",
    "    if 'children' not in root:\n",
    "        if 'text' in root and 'href' in root:\n",
    "            return (root['text'],'' if root['href'] == 'javascript:void(0);' else root['href'],1)\n",
    "            # Aesthetic output\n",
    "            # return (f\"root['text']}-> {root['href']}\",1)\n",
    "        # else:\n",
    "        #     return ['','',0]\n",
    "    else:\n",
    "        for child in root['children']:\n",
    "            # print(child)\n",
    "            links = bs4_convert_tree(child)\n",
    "            if links:\n",
    "                # print(links)\n",
    "                total_hrefs += links[-1]\n",
    "                ans += [links[:-1]]\n",
    "            \n",
    "    return [*list(filter(None,ans)),total_hrefs]\n",
    "\n",
    "# Need to handle javascript:void(0); case\n",
    "def bs4_assess_href(base_url, href) -> str:\n",
    "    if not validators.url(href):\n",
    "        href = up.urljoin(base_url,href)\n",
    "    return [href, 'relevant' if up.urlparse(href).netloc == up.urlparse(base_url).netloc else 'irrelevant']\n",
    "\n",
    "def bs4_find_relevant_hrefs(soup, website_url: str) -> list[tuple[str, str]]:\n",
    "    atags = soup.find_all('a')\n",
    "    relevant_hrefs = []\n",
    "    for a in atags:\n",
    "        text, href = a.get_text(strip=True),a.get('href')\n",
    "        assessed_href = bs4_assess_href(website_url, href)\n",
    "        if assessed_href[1] == 'relevant' and (assessed_href[0] != website_url and text != 'Skip to content'):\n",
    "            relevant_hrefs += [(text, assessed_href[0])]\n",
    "    return relevant_hrefs\n",
    "\n",
    "def bs4_find_first_href(home_page_url, nested_list) -> str:\n",
    "    for item in nested_list:    \n",
    "        if isinstance(item, list):\n",
    "                    # Recursively search within the list\n",
    "                    result = bs4_find_first_href(home_page_url, item)\n",
    "                    if result:  # If a valid URL is found in the recursion, return it\n",
    "                        return result\n",
    "        elif isinstance(item, tuple) and len(item) == 2:\n",
    "            if validators.url(item[1]) and item[1] != home_page_url:\n",
    "                return item[1]  # Return the URL if it's valid\n",
    "    return 'No href found'\n",
    "\n",
    "def bs4_nav_scrape(website_url: str, soup) -> list[list[tuple[str,str]],str]:\n",
    "    bs4_nav_return, nav_trees = [], []\n",
    "\n",
    "    # Find navs, construct trees, find max\n",
    "    navs = soup.find_all('nav')\n",
    "    for nav in navs:\n",
    "        nav_trees.append(bs4_build_tree(website_url, nav))\n",
    "    max_nav = ({},0)\n",
    "    for tree in nav_trees:\n",
    "        converted = bs4_convert_tree(tree)\n",
    "        if converted[-1] > max_nav[-1]:\n",
    "            max_nav = converted\n",
    "    # [:-1] to account for nested tree\n",
    "    bs4_max_nav_tree = max_nav[:-1]\n",
    "\n",
    "    # Construct return\n",
    "    if not max_nav[0]: #or len(max_nav[-1]) < x:\n",
    "        # If no/not enough navs, find all relevant atags\n",
    "        relevant_hrefs = bs4_find_relevant_hrefs(soup, website_url)\n",
    "        bs4_nav_return.append(relevant_hrefs)\n",
    "        first_href = relevant_hrefs[0][1] if relevant_hrefs else ''\n",
    "    else:\n",
    "        bs4_nav_return.append(bs4_max_nav_tree)\n",
    "        first_href = bs4_find_first_href(website_url, max_nav)\n",
    "\n",
    "    bs4_nav_return.append(first_href)\n",
    "    return bs4_nav_return\n",
    "\n",
    "# url = 'https://www.dentalxchange.com/'\n",
    "# url = 'https://ecmins.com/'\n",
    "# url = 'https://iquartic.com/' # blocked on requests\n",
    "# url = 'https://www.ripoffreportremovalhelp.com/' # blocked on requests\n",
    "# url = 'https://pulseca.com/'\n",
    "url_test = 'https://www.scorpion.co/'\n",
    "\n",
    "html = requests.get(url_test).content\n",
    "soupt = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# url_test = 'https://www.pavestone.com/'\n",
    "response = requests.get(url_test)\n",
    "soupy = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# response = requests.get(url_test).content\n",
    "# soupr = BeautifulSoup(response, 'html.parser')\n",
    "bs4_nav_scrape(url_test, soupy)\n",
    "# soupy.find_all('a')\n",
    "# print(soupr.find('h2'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765fd70b",
   "metadata": {},
   "source": [
    "**Report:** it seems as though the javascript:void(0); case is handled because validators.url thinks it's valid, but the netloc's are not the same, so it's labelled irrelevant. \\\n",
    "**TODO:** Need to figure out nav name (the text only in the nav element, not in the contained a's, create tree-like structure. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751820c5",
   "metadata": {},
   "source": [
    "### Scraping Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dedc3323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(seg):\n",
    "    count = 0\n",
    "    for i in seg:\n",
    "        if i == ' ':\n",
    "            count += 1\n",
    "    return count+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d9b9e3",
   "metadata": {},
   "source": [
    "#### bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "26895bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.forsalebyowner.com/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'BS4 Nav list unavailable'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bs4_pages_scrape(urls: list[str]) -> list[dict]:\n",
    "    pages = []\n",
    "    for url in urls:\n",
    "        if url and validators.url(url):\n",
    "            try:\n",
    "                response = requests.get(url).text\n",
    "            except Exception as e:\n",
    "                print(f'HTTPRequest error: {e}')\n",
    "                pages.append({'headers':['Page not available']})\n",
    "                return pages\n",
    "            soup = BeautifulSoup(response, 'html.parser')\n",
    "            # Split on any whitespace (\\n and \\t) -> maybe this is causing weird headers\n",
    "            page_text = soup.get_text(\"|\",strip=True).split(\"|\")\n",
    "            # Extract the first two pieces of text with more than (7) words -> to be tested\n",
    "            first_relevant = {'first_relevant': [i for i in page_text if word_count(i) > 7][:2]}\n",
    "            # Two longest pieces of text on the page. Test if this produces relevant results\n",
    "            two_longest = {'two_longest': sorted(page_text,key=len)[-2:]}\n",
    "            # Find all h1s and h2s\n",
    "            h1s = soup.find_all('h1')\n",
    "            h2s = soup.find_all('h2')\n",
    "            h1_texts = [h1.get_text(strip=True) for h1 in h1s]\n",
    "            h2_texts = [h2.get_text(strip=True) for h2 in h2s]\n",
    "            headers = {'headers': list(filter(None,h1_texts+h2_texts))}\n",
    "            pages.append({**first_relevant, **two_longest,**headers})\n",
    "        else:\n",
    "            pages.append({'headers':['Page not available']})\n",
    "\n",
    "    return pages\n",
    "\n",
    "# Takes response_text instead of a URL since the request is required to determine bs4/sel\n",
    "def bs4_scrape(website_url: str, response_text: str) -> dict[str,str|dict[str,str]]:\n",
    "    soup = BeautifulSoup(response_text,'html.parser')\n",
    "    url_results = {}\n",
    "\n",
    "    #Scrape nav\n",
    "    nav_list = bs4_nav_scrape(website_url, soup)\n",
    "    if not nav_list[0]: return 'BS4 Nav list unavailable'\n",
    "    url_results['nav'] = nav_list\n",
    "\n",
    "    # Scrape home page and if there, first page\n",
    "    urls = [website_url]\n",
    "    if nav_list[1] and validators.url(nav_list[1]):\n",
    "        urls.append(nav_list[1])\n",
    "    pages = bs4_pages_scrape(urls)\n",
    "    home_page_obj = pages[0]\n",
    "    first_page_obj = pages[1] if len(pages) > 1 else {'headers':['First page unavailable']}\n",
    "    url_results['home_page'], url_results['first_page'] = home_page_obj, first_page_obj\n",
    "\n",
    "    # Extract headers\n",
    "    url_results['headers'] = home_page_obj['headers'] + first_page_obj['headers']\n",
    "\n",
    "    return url_results\n",
    "\n",
    "urler = 'https://www.forsalebyowner.com/'\n",
    "response = requests.get(urler)\n",
    "print(response.url)\n",
    "souper = BeautifulSoup(response.content, 'html.parser')\n",
    "bs4_scrape(urler, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c5d519",
   "metadata": {},
   "source": [
    "#### Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1d8b51f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathers first two relevant chunks of texts, two longest chunks of text and all h1s and h2s from every url in list then closes stealth driver fed in\n",
    "def sel_pages_scrape(driver, urls: list[str]) -> dict:\n",
    "    pages = []\n",
    "    for url in urls:\n",
    "        if url and validators.url(url):\n",
    "            driver.get(url)    \n",
    "            time.sleep(2)\n",
    "            page_text = driver.find_element(\"xpath\",\"/html/body\").text\n",
    "            # Split on any whitespace (\\n and \\t)\n",
    "            page_array = re.split(r'[\\n\\t]+',page_text)\n",
    "            # Extract the first two pieces of text with more than (7) words -> to be tested\n",
    "            first_relevant = {'first_relevant': [i for i in page_array if word_count(i) > 7][:2]}\n",
    "            # Two longest pieces of text on the page. Test if this produces relevant results\n",
    "            two_longest = {'two_longest': sorted(page_array,key=len)[-2:]}\n",
    "            h1s = driver.find_elements(\"xpath\",\"//h1\")\n",
    "            h2s = driver.find_elements(\"xpath\",\"//h2\")\n",
    "            h1_texts = [h1.text for h1 in h1s if h1]\n",
    "            h2_texts = [h2.text for h2 in h2s if h2]\n",
    "            headers = {'headers':list(filter(None,h1_texts+h2_texts))}\n",
    "            pages.append({**first_relevant, **two_longest, **headers})\n",
    "        else:\n",
    "            pages.append({'headers':['First page unavailable']})\n",
    "    # driver.close()\n",
    "    return pages\n",
    "\n",
    "def sel_scrape(url: str) -> dict[str,str|dict[str,str]]:\n",
    "    print(f'Selenium scraping {url}')\n",
    "    url_results = {}\n",
    "\n",
    "    #Scrape nav\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    nav_list = sel_nav_scrape(driver)\n",
    "    driver.close()\n",
    "    url_results['nav'] = nav_list\n",
    "    print(f'{up.urlparse(url).netloc} sel naver', nav_list)\n",
    "\n",
    "    # Configure driver to be passed throughout\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    stealth_driver = webdriver.Chrome(options=options)\n",
    "    stealth(stealth_driver,\n",
    "            languages=[\"en-US\", \"en\"],\n",
    "            vendor=\"Google Inc.\",\n",
    "            platform=\"Win32\",\n",
    "            webgl_vendor=\"Intel Inc.\",\n",
    "            renderer=\"Intel Iris OpenGL Engine\",\n",
    "            fix_hairline=True,\n",
    "            )\n",
    "    stealth_driver.set_window_size(1100, 720)\n",
    "    # stealth_driver.get(url)\n",
    "    \n",
    "    # Scrape home and first pages (requires both of these to have urls).\n",
    "    home_page_obj, first_page_obj = sel_pages_scrape(stealth_driver, [url, nav_list[1]])\n",
    "    url_results['home_page'], url_results['first_page'] = home_page_obj, first_page_obj\n",
    "\n",
    "    # Extract headers\n",
    "    url_results['headers'] = home_page_obj['headers'] + first_page_obj['headers']\n",
    "\n",
    "    # stealth_driver.close()\n",
    "    \n",
    "    return url_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf66b609",
   "metadata": {},
   "source": [
    "**Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f11d175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.appliedlearning.com\n",
    "# https://sunstonepartners.com\n",
    "# https://ecmins.com\n",
    "# https://www.dentalxchange.com/\n",
    "# https://pulseca.com/\n",
    "# https://cessco.ca/\n",
    "# sel_scrape('https://www.dentalxchange.com/')\n",
    "# options = Options()\n",
    "\n",
    "nav_driver = webdriver.Chrome()\n",
    "sel_pages_scrape(nav_driver,['https://www.forsalebyowner.com/','https://www.forsalebyowner.com/sellyourhome/package'])\n",
    "nav_driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a57b706",
   "metadata": {},
   "source": [
    "#### Ancillary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1d4b136b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_processing(url):\n",
    "    if not url or url != url or pd.isna(url):\n",
    "        return ''\n",
    "    \n",
    "    # Sanitize URL\n",
    "    corrected_url = sanitize_url(url)\n",
    "    return corrected_url\n",
    "\n",
    "# Function to sanitize/correct URLs missing pieces\n",
    "def sanitize_url(url):\n",
    "    # Parse URL to correct any issues then reconstruct\n",
    "    parsed_url = urlparse(url)\n",
    "\n",
    "    if not parsed_url.scheme:\n",
    "    # Assume http scheme\n",
    "        corrected_url = 'http://'+parsed_url.netloc + parsed_url.path + parsed_url.params + parsed_url.query + parsed_url.fragment\n",
    "    else:\n",
    "        corrected_url = parsed_url.geturl()\n",
    "\n",
    "    return corrected_url\n",
    "\n",
    "async def check_url(session, url, semaphore):\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            async with session.head(url, allow_redirects=True, timeout=100) as response:\n",
    "                return str(response.url) # Return final URL as string\n",
    "        # Catch errors\n",
    "        except asyncio.TimeoutError as te:\n",
    "            return 'Timeout Error'\n",
    "        except aiohttp.ClientError as ce:\n",
    "            return 'Client Error'\n",
    "        except ValueError as ve:\n",
    "            return 'Value Error'\n",
    "        \n",
    "async def capture_url_redirects(urls, MAX_CONCURRENT_REQUESTS):\n",
    "    print(f\"processing {len(urls)} urls\")\n",
    "    semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [check_url(session, url, semaphore) for url in urls]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        return results\n",
    "    \n",
    "async def return_invalid_url_object(url) -> dict[str, str | dict | list]:\n",
    "    return {'website_redirect': url, 'nav': 'Invalid_URL','home_page':{},'first_page':{},'headers':[]}\n",
    "\n",
    "# # Specify the desired Chromium version\n",
    "# os.environ['PYPPETEER_CHROMIUM_REVISION'] = '1263111'\n",
    "# chromium_revision = os.getenv('PYPPETEER_CHROMIUM_REVISION', 'Environment variable not set')\n",
    "# print('sdfsdf',chromium_revision)\n",
    "# print(pyppeteer.__chromium_revision__)\n",
    "async def pypp_scrape(url):\n",
    "    print(f'pypp_scrape {url}')\n",
    "    browser = await launch(headless=True, executablePath='C:/Users/leogr/AppData/Local/pyppeteer/pyppeteer/chrome-win/chrome-win/chrome.exe')\n",
    "    page = await browser.newPage()\n",
    "    # start_time = time.time()\n",
    "    await page.goto(url)\n",
    "    # elapsed_time = time.time() - start_time\n",
    "    cookies = await page.cookies()\n",
    "    # print(f\"Page loaded in {elapsed_time} seconds.\")\n",
    "    await browser.close()\n",
    "    return cookies\n",
    "\n",
    "# async def download_chromium():\n",
    "#     browser = await launch()\n",
    "#     await browser.close()\n",
    "\n",
    "# print(pyppeteer.__chromium_revision__)\n",
    "\n",
    "# asyncio.run(download_chromium())\n",
    "\n",
    "# url = 'https://cessco.ca/'\n",
    "# # url = 'https://ecmins.com/'\n",
    "# asyncio.run(load_page(url))\n",
    "\n",
    "def update_redirect_urls(file_path, index_range, redirect_urls):\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "    new_list = list(df['Website Redirect'][:index_range.start]) + redirect_urls + list(df['Website Redirect'][index_range.stop:])\n",
    "    df['Website Redirect'] = new_list\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "def construct_df_col(df, col_name: str, scrape_col: list, index_range: slice, col_exists: bool):\n",
    "    if col_exists:\n",
    "        return list(df[col_name][:index_range.start]) + scrape_col + list(df[col_name][index_range.stop:])\n",
    "    else:\n",
    "        return ['']*index_range.start + scrape_col + ['']*(len(df)-index_range.stop)\n",
    "\n",
    "def update_scrape_results(file_path: str, scrape_results: list[dict], index_range: slice):\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "    print('TYPE',type(scrape_results),type(scrape_results[0]))\n",
    "    # TODO: Add 'Metadata' here when ready\n",
    "    for column in ['Website Redirect','Nav','Headers','Home Page','First Page']:\n",
    "        isolated_col = [result['_'.join(column.lower().split(' '))] for result in scrape_results]\n",
    "        df[column] = construct_df_col(df, column, isolated_col, index_range, column in df)\n",
    "    df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76454dac",
   "metadata": {},
   "source": [
    "### Threaded Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1983d1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://cessco.ca'] [nan] False\n",
      "plp 0 False False nan\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 232\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds. Excel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex_range\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m updated.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    231\u001b[0m \u001b[38;5;66;03m# loop = asyncio.get_event_loop()\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthreaded_main\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m239\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m240\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\jupyter_env\\Lib\\asyncio\\futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\jupyter_env\\Lib\\asyncio\\tasks.py:277\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "Cell \u001b[1;32mIn[60], line 196\u001b[0m, in \u001b[0;36mthreaded_main\u001b[1;34m(start, stop)\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m redirect_url:\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplp\u001b[39m\u001b[38;5;124m'\u001b[39m,i, redirect_url \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, redirect_url \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28mstr\u001b[39m(redirect_url))\n\u001b[1;32m--> 196\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m redirect_url \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mvalidators\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mredirect_url\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    197\u001b[0m         raw_urls[i] \u001b[38;5;241m=\u001b[39m redirect_url\n\u001b[0;32m    199\u001b[0m sanitized_urls \u001b[38;5;241m=\u001b[39m [initial_processing(url) \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m raw_urls]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\jupyter_env\\Lib\\site-packages\\validators\\utils.py:83\u001b[0m, in \u001b[0;36mvalidator.<locals>.wrapper\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 83\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value:\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ValidationFailure(\n\u001b[0;32m     86\u001b[0m             func, func_args_as_dict(func, args, kwargs)\n\u001b[0;32m     87\u001b[0m         )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\jupyter_env\\Lib\\site-packages\\validators\\url.py:148\u001b[0m, in \u001b[0;36murl\u001b[1;34m(value, public)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@validator\u001b[39m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21murl\u001b[39m(value, public\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    Return whether or not given value is a valid URL.\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;124;03m    :param public: (default=False) Set True to only allow a public IP address\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 148\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mpattern\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m public:\n\u001b[0;32m    150\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object, got 'float'"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "import validators\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pyppeteer import launch\n",
    "import pyppeteer\n",
    "import os\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize Selenium drivers for each type of task\n",
    "async def init_driver_pool(size):\n",
    "    queue = asyncio.Queue(maxsize=size)\n",
    "    for _ in range(size):\n",
    "        options = webdriver.ChromeOptions()\n",
    "        # options.add_argument(\"--start-maximized\")\n",
    "        stealth_driver = webdriver.Chrome(options=options)\n",
    "        stealth(stealth_driver,\n",
    "                languages=[\"en-US\", \"en\"],\n",
    "                vendor=\"Google Inc.\",\n",
    "                platform=\"Win32\",\n",
    "                webgl_vendor=\"Intel Inc.\",\n",
    "                renderer=\"Intel Iris OpenGL Engine\",\n",
    "                fix_hairline=True,\n",
    "                )\n",
    "        # stealth_driver.set_window_size(1100, 720)\n",
    "        await queue.put(stealth_driver)\n",
    "    return queue\n",
    "\n",
    "# Close all drivers in the pool\n",
    "async def close_driver_pool(driver_pool):\n",
    "    while not driver_pool.empty():\n",
    "        driver = await driver_pool.get()\n",
    "        driver.quit()\n",
    "        driver_pool.task_done()\n",
    "\n",
    "async def capture_redirect(session, url, headers, semaphore, executor) -> list[str,str]:\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            # First, attempt to scrape using aiohttp\n",
    "            async with session.get(url, allow_redirects=True, headers=headers, timeout=150) as response:\n",
    "                if response.status//100 == 2:\n",
    "                    response_text = await response.text()\n",
    "                    bs4_result = bs4_scrape(url, response_text)\n",
    "                    if bs4_result == 'BS4 Nav list unavailable':\n",
    "                        # Define no BS4 nav as 600 error\n",
    "                        raise Exception('BS4 doesn\\'t know where to go -> 600')\n",
    "                    return [response.url, 'bs4',bs4_result]\n",
    "                else:\n",
    "                    raise Exception(f\"Non-200 response -> {response.status}\")\n",
    "        # TODO: Catch errors better (cessco)\n",
    "        except asyncio.TimeoutError as te:\n",
    "            # TODO: go back and selenium all of these with longer timeout, returning invalid to get through\n",
    "            return ['Timeout_Error', 'invalid']\n",
    "        except aiohttp.ClientError as ce:\n",
    "            return ['Client_Error', 'invalid']\n",
    "        except ValueError as ve:\n",
    "            return ['Value_Error', 'invalid']\n",
    "        except Exception as e:\n",
    "            print(f'Error with {url}: {e}')\n",
    "            try:\n",
    "                error_code = int(str(e).split(' ')[-1])\n",
    "                # TODO: figure out 464 error for hellohero\n",
    "                if type(error_code == int) and (error_code // 100 == 5 or error_code == 404):\n",
    "                    return ['Invalid_URL', 'invalid']\n",
    "            except Exception as e:\n",
    "                raise Exception(f'Error with exception: {e}')\n",
    "            # Fallback to Selenium scraping within the thread pool executor\n",
    "            return ['Pyppeteer','pyppeteer']\n",
    "            # return [url, 'selenium']\n",
    "\n",
    "async def nav_scrape(final_url, session, semaphore, executor, driver_pool) -> list[list[tuple[str, str]],str]:\n",
    "    nav_driver = await driver_pool.get()\n",
    "    nav_driver.get(final_url)\n",
    "    ret = sel_nav_scrape(nav_driver)\n",
    "    await driver_pool.put(nav_driver)\n",
    "    return ret\n",
    "\n",
    "async def home_page_scrape(final_url, session, semaphore, executor, driver_pool):\n",
    "    home_driver = await driver_pool.get()\n",
    "    home_driver.get(final_url)\n",
    "    ret = sel_pages_scrape(home_driver, [final_url])[0]\n",
    "    await driver_pool.put(home_driver)\n",
    "    return ret\n",
    "\n",
    "async def first_page_scrape(first_url, session, semaphore, executor, driver_pool):\n",
    "    first_driver = await driver_pool.get()\n",
    "    if validators.url(first_url):\n",
    "        first_driver.get(first_url)\n",
    "    ret = sel_pages_scrape(first_driver, [first_url])[0]\n",
    "    await driver_pool.put(first_driver)\n",
    "    return ret\n",
    "\n",
    "# Coordination point of website scraping\n",
    "async def scrape_url_async(session, url, driver_pool):\n",
    "    print(f'Starting {url} scrape.')\n",
    "    MAX_CONCURRENT_REQUESTS, MAX_WORKERS = 1000, 3\n",
    "    semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)\n",
    "    executor = ThreadPoolExecutor(max_workers=MAX_WORKERS)\n",
    "\n",
    "    # Assemble headers\n",
    "    headers = {\n",
    "        'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36 Edg/122.0.0.0',\n",
    "        'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "        'Referer':url\n",
    "    }\n",
    "    \n",
    "    redirect_task = asyncio.create_task(capture_redirect(session, url, headers, semaphore, executor))\n",
    "    redirect_return = await redirect_task\n",
    "    final_url, scrape_type = redirect_return[:2]\n",
    "    print('stype',scrape_type)\n",
    "    # bs4_result acquired\n",
    "    if scrape_type == 'bs4':\n",
    "        print(f'{url} processed by bs4.')\n",
    "        return {'website_redirect': final_url, **redirect_return[2]}\n",
    "    elif scrape_type == 'invalid':\n",
    "        print(f'{url} processed. Invalid: {final_url}.')\n",
    "        return await return_invalid_url_object(final_url)\n",
    "\n",
    "    # TODO: What are the repsonses when it's invalid vs. pypp?\n",
    "\n",
    "    # TODO: pypp gets home page text?\n",
    "    pypp_task = asyncio.create_task(pypp_scrape(url))\n",
    "    pypp_return = await pypp_task\n",
    "    # headers['Cookie'] = asemble_relevant_cookie(pypp_return)\n",
    "    headers['Cookie'] = 'sd_fw_data=3f877dcf6ce2b0cd5ff8421da7101cb0|1|IN78Nl9dz9599|V2luMzJ8ZmFsc2V8ZW4tVVN8NS4wIChXaW5kb3dzIE5UIDEwLjA7IFdpbjY0OyB4NjQpIEFwcGxlV2ViS2l0LzUzNy4zNiAoS0hUTUwsIGxpa2UgR2Vja28pIENocm9tZS8xMjIuMC4wLjAgU2FmYXJpLzUzNy4zNiBFZGcvMTIyLjAuMC4wfGZhbHNlfDEwfDh8dHJ1ZXx8OHx8MTI3Mnw1NjR8MTI4MHw2NzJ8M3xlbi1VUyxlbnwzM3w1fDB8MnwxMjgwfDY3MnwyNHwyNHw1Ni4xMTI0NDc4MTY4NjE0fGZhbHNlfGZhbHNlfHRydWV8ZmFsc2V8QU5HTEUgKEludGVsLCBJbnRlbChSKSBVSEQgR3JhcGhpY3MgKDB4MDAwMDlCNDEpIERpcmVjdDNEMTEgdnNfNV8wIHBzXzVfMCwgRDNEMTEpfDM5Njl8V2luZG93c3xmYWxzZXxDaHJvbWl1bToxMjIsTm90KEE6QnJhbmQ6MjQsTWljcm9zb2Z0IEVkZ2U6MTIyfHRydWV8dHJ1ZXw1NXw1OXxBbWVyaWNhL0xvc19BbmdlbGVzfDF8MXwxfDE1LjAuMHwxMjIuMC42MjYxLjk1fHxkZWZhdWx0fHByb21wdHw3MTR8MTM3NTh8NjYyMnw4MXx8NzY4fDU4MTB8fDg4fDE3MDM2MjY1MzV8MTcwOTUwNjg2OHxuQjBJYTRCQTdDdjVTTnZycTJDSXx8',\n",
    "\n",
    "    redirect_retask = asyncio.create_task(capture_redirect(session, url, headers, semaphore, executor))\n",
    "    redirect_rereturn = await redirect_retask\n",
    "    final_url, scrape_type = redirect_rereturn[:2]\n",
    "    print('wht',final_url,scrape_type)\n",
    "    # bs4_result acquired\n",
    "    if scrape_type == 'bs4':\n",
    "        print(f'{url} processed by bs4.')\n",
    "        return {'website_redirect': final_url, **redirect_return[2]}\n",
    "    elif scrape_type == 'invalid':\n",
    "        print(f'{url} processed. Invalid: {final_url}.')\n",
    "        return await return_invalid_url_object(final_url)\n",
    "\n",
    "\n",
    "    \n",
    "    print('pypper',pypp_return)\n",
    "    return await return_invalid_url_object(url)\n",
    "\n",
    "    \n",
    "    loop = asyncio.get_running_loop()\n",
    "    nav_task = asyncio.create_task(nav_scrape(final_url, session, semaphore, executor, driver_pool))\n",
    "    # nav_task = loop.run_in_executor(executor, nav_scrape, final_url, session, semaphore, executor, driver_pool)\n",
    "    home_task = loop.run_in_executor(executor, home_page_scrape, final_url, session, semaphore, executor,driver_pool)\n",
    "\n",
    "    # Handle first page scrape\n",
    "    # Await nav_task to ensure nav_info is available for first_page_scrape\n",
    "    nav_info = await nav_task\n",
    "    first_page_url = nav_info[-1]\n",
    "    first_page_data, home_page_data = {}, {}\n",
    "    if validators.url(first_page_url):\n",
    "        # first_page_task = asyncio.create_task(first_page_scrape(nav_info[-1], session, semaphore, executor, driver_pool))\n",
    "        first_page_task = loop.run_in_executor(executor, first_page_scrape, nav_info[-1], session, semaphore, executor, driver_pool)\n",
    "    \n",
    "        # Await all tasks and collect results\n",
    "        home_page_data, first_page_data = await asyncio.gather(await home_task, await first_page_task)\n",
    "    else:\n",
    "        first_page_data = {'headers':'No first page found'}\n",
    "    # TODO: unnecessary because the data is already there?\n",
    "    # headers = home_page_data['headers'] + first_page_data['headers']\n",
    "    print(f'{url} processed by sel.')\n",
    "    return {'website_redirect': final_url,'nav':nav_info[:-1], 'home_page':home_page_data, 'first_page': first_page_data, 'headers':[]} #, 'headers':headers}\n",
    "\n",
    "async def threaded_main(start, stop):\n",
    "    start_time = time.time()\n",
    "    MAX_DRIVERS = 12\n",
    "\n",
    "    # Excel index = 2 + this index\n",
    "    # start, stop = 150,200\n",
    "    if stop <= start:\n",
    "        print('Start must be strictly less than stop')\n",
    "        return 1\n",
    "    index_range = slice(start, stop)\n",
    "\n",
    "    # Load your URLs from a file or list\n",
    "    file_path = './Excel_Sheets/Website_Redirects_230919.csv'\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "    raw_urls = df['Website'][index_range].tolist()\n",
    "    if 'Website Redirect' in df:\n",
    "        redirect_urls = df.get('Website Redirect', pd.Series(dtype=str)).tolist()[index_range]\n",
    "\n",
    "    print(raw_urls, redirect_urls, redirect_urls[0] == True)\n",
    "    \n",
    "    # Check if 'Website Redirect' column is already populated (with valid URL)\n",
    "    for i, redirect_url in enumerate(redirect_urls):\n",
    "        if redirect_url:\n",
    "            print('plp',i, redirect_url == False, redirect_url == True, str(redirect_url) == True)\n",
    "        if redirect_url and validators.url(redirect_url):\n",
    "            raw_urls[i] = redirect_url\n",
    "\n",
    "    sanitized_urls = [initial_processing(url) for url in raw_urls]\n",
    "    valid_urls = [url if validators.url(url) else '' for url in sanitized_urls]\n",
    "\n",
    "    scrape_tasks = []\n",
    "    # driver_pool = await init_driver_pool(MAX_DRIVERS)\n",
    "    driver_pool = []\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        scrape_tasks = [scrape_url_async(session, url, driver_pool) for url in valid_urls]\n",
    "        scrape_results = await asyncio.gather(*scrape_tasks)\n",
    "\n",
    "        # returns single scrape_result\n",
    "    #     scrape_task = asyncio.create_task(scrape_url_async(session, url, driver_pool))\n",
    "    #     scrape_tasks.append(scrape_task)\n",
    "    # scrape_results = await asyncio.gather(*scrape_tasks)\n",
    "        \n",
    "    print('closing pool')\n",
    "    count = 0\n",
    "\n",
    "    # await close_driver_pool(driver_pool)\n",
    "    for i in scrape_results:\n",
    "        if i['nav'] == 'Invalid_URL' or (type(i['website_redirect']) == str and 'Error' in i['website_redirect']):\n",
    "            count+=1\n",
    "    print('scrrr',count, type(scrape_results),type(scrape_results[0]), scrape_results)\n",
    "\n",
    "    # loop = asyncio.get_event_loop()\n",
    "    # scrape_results = loop.run_until_complete(main_async(valid_urls))\n",
    "\n",
    "    update_scrape_results(file_path, scrape_results, index_range)\n",
    "\n",
    "    print(f\"Completed in {time.time() - start_time} seconds. Excel {index_range} updated.\")\n",
    "\n",
    "# loop = asyncio.get_event_loop()\n",
    "asyncio.run(threaded_main(239,240))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dc3eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9,10):\n",
    "    asyncio.run(threaded_main(i*50,(i+1)*50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50295a63",
   "metadata": {},
   "source": [
    "### Handle SINGULARITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb550e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_scrape('https://www.cosmonetsolutions.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346b71f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver = webdriver.Chrome()\n",
    "# driver.get('https://hellohero.com')\n",
    "x,y = 9,10\n",
    "tost_pg_res = [{'website_redirect': 'https://21stsoft.com', **sel_scrape('https://21stsoft.com')}]\n",
    "file_path = './Excel_Sheets/Website_Redirects_230919.csv'\n",
    "update_scrape_results(file_path,tost_pg_res, slice(x,y))\n",
    "\n",
    "# driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86297352",
   "metadata": {},
   "source": [
    "aiohttp response different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d85a8c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [403]>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get('https://www.cosmonetsolutions.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017e2849-a34e-4ed5-99cd-e558d2347bff",
   "metadata": {},
   "source": [
    "**Pyppeteer/Playwright**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b52717d-a83e-41ff-8ece-aa53a227167a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdfsdf 1263111\n",
      "1181205\n",
      "Page loaded in 0.18077754974365234 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'sd_fw_data',\n",
       "  'value': '3f877dcf6ce2b0cd5ff8421da7101cb0|1|IN78Nl9dz9599|fHx8fHx8fHx8fHx8fHx8fHx8fHx8fHx8fHx8fHx8fHx8fHx8fHx8fHx8fHx8fHx8fHx8fHx8fHwxfDE3MDk3NDM5NTR8MTcwOTc0Mzk1NHxmdjNvd3RxTk9scXBvN09aVmRMT3x8',\n",
       "  'domain': 'cessco.ca',\n",
       "  'path': '/',\n",
       "  'expires': 1712335954,\n",
       "  'size': 195,\n",
       "  'httpOnly': False,\n",
       "  'secure': True,\n",
       "  'session': False,\n",
       "  'priority': 'Medium',\n",
       "  'sameParty': False,\n",
       "  'sourceScheme': 'Secure',\n",
       "  'sourcePort': 443}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyppeteer import launch\n",
    "import pyppeteer\n",
    "import asyncio\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Specify the desired Chromium version\n",
    "os.environ['PYPPETEER_CHROMIUM_REVISION'] = '1263111'\n",
    "chromium_revision = os.getenv('PYPPETEER_CHROMIUM_REVISION', 'Environment variable not set')\n",
    "print('sdfsdf',chromium_revision)\n",
    "\n",
    "print(pyppeteer.__chromium_revision__)\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def load_page(url):\n",
    "    browser = await launch(headless=True, executablePath='C:/Users/leogr/AppData/Local/pyppeteer/pyppeteer/chrome-win/chrome-win/chrome.exe')\n",
    "    page = await browser.newPage()\n",
    "    start_time = time.time()\n",
    "    await page.goto(url)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    cookies = await page.cookies()\n",
    "    print(f\"Page loaded in {elapsed_time} seconds.\")\n",
    "    await browser.close()\n",
    "    return cookies\n",
    "\n",
    "async def download_chromium():\n",
    "    browser = await launch()\n",
    "    await browser.close()\n",
    "\n",
    "# print(pyppeteer.__chromium_revision__)\n",
    "\n",
    "# asyncio.run(download_chromium())\n",
    "\n",
    "url = 'https://cessco.ca/'\n",
    "# url = 'https://ecmins.com/'\n",
    "asyncio.run(load_page(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6e2364-78d9-4f57-922f-d7f274082cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from playwright.async_api import async_playwright\n",
    "\n",
    "async def tester():\n",
    "    with async_playwright() as p:\n",
    "        print(p)\n",
    "\n",
    "t = await tester()\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd932e96-899f-44de-8e64-98310d72b534",
   "metadata": {},
   "outputs": [],
   "source": [
    "urlh = 'https://cessco.ca/'\n",
    "\n",
    "async with aiohttp.ClientSession() as session:\n",
    "     async with session.get(urlh, allow_redirects=True, timeout=50) as response:\n",
    "        print('sta',response.status)\n",
    "\n",
    "        # print('STATUS', response.status,type(response.text()),type(response.text),type(response))\n",
    "        if response.status == 200:\n",
    "            # bs4_soup = BeautifulSoup(response.text(), 'html.parser')\n",
    "            # print('BS4 result',bs4_soup)\n",
    "            # response.text() coroutine for asynchroneity\n",
    "            response_text = await response.text()\n",
    "            print('r',type(response_text), response_text)\n",
    "            bs4_result = bs4_scrape(urlh, response_text)\n",
    "\n",
    "        try:\n",
    "            response = requests.get(urlh)\n",
    "            # res = bs4_scrape(urlh,response.text)\n",
    "            soup1 = BeautifulSoup(response.text, 'html.parser')\n",
    "            # soup2 = BeautifulSoup(await response.text(), 'html.parser')\n",
    "            bs5_result = bs4_scrape(urlh, response.text)\n",
    "            # print(res)\n",
    "        except Exception as e:\n",
    "            print('fail', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6275654e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientConnectorCertificateError",
     "evalue": "Cannot connect to host www.cosmonetsolutions.com:443 ssl:True [SSLCertVerificationError: (1, \"[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for 'www.cosmonetsolutions.com'. (_ssl.c:992)\")]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSSLCertVerificationError\u001b[0m                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\aiohttp\\connector.py:992\u001b[0m, in \u001b[0;36mTCPConnector._wrap_create_connection\u001b[1;34m(self, req, timeout, client_error, *args, **kwargs)\u001b[0m\n\u001b[0;32m    989\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m ceil_timeout(\n\u001b[0;32m    990\u001b[0m         timeout\u001b[38;5;241m.\u001b[39msock_connect, ceil_threshold\u001b[38;5;241m=\u001b[39mtimeout\u001b[38;5;241m.\u001b[39mceil_threshold\n\u001b[0;32m    991\u001b[0m     ):\n\u001b[1;32m--> 992\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop\u001b[38;5;241m.\u001b[39mcreate_connection(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    993\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m cert_errors \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\asyncio\\base_events.py:1098\u001b[0m, in \u001b[0;36mBaseEventLoop.create_connection\u001b[1;34m(self, protocol_factory, host, port, ssl, family, proto, flags, sock, local_addr, server_hostname, ssl_handshake_timeout, ssl_shutdown_timeout, happy_eyeballs_delay, interleave)\u001b[0m\n\u001b[0;32m   1095\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1096\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA Stream Socket was expected, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msock\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1098\u001b[0m transport, protocol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_transport(\n\u001b[0;32m   1099\u001b[0m     sock, protocol_factory, ssl, server_hostname,\n\u001b[0;32m   1100\u001b[0m     ssl_handshake_timeout\u001b[38;5;241m=\u001b[39mssl_handshake_timeout,\n\u001b[0;32m   1101\u001b[0m     ssl_shutdown_timeout\u001b[38;5;241m=\u001b[39mssl_shutdown_timeout)\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_debug:\n\u001b[0;32m   1103\u001b[0m     \u001b[38;5;66;03m# Get the socket from the transport because SSL transport closes\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m     \u001b[38;5;66;03m# the old socket and creates a new SSL socket\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\asyncio\\base_events.py:1131\u001b[0m, in \u001b[0;36mBaseEventLoop._create_connection_transport\u001b[1;34m(self, sock, protocol_factory, ssl, server_hostname, server_side, ssl_handshake_timeout, ssl_shutdown_timeout)\u001b[0m\n\u001b[0;32m   1130\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1131\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m waiter\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\asyncio\\futures.py:287\u001b[0m, in \u001b[0;36mFuture.__await__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_asyncio_future_blocking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 287\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\asyncio\\tasks.py:339\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 339\u001b[0m     \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\asyncio\\futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\asyncio\\sslproto.py:577\u001b[0m, in \u001b[0;36mSSLProtocol._on_handshake_complete\u001b[1;34m(self, handshake_exc)\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 577\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m handshake_exc\n\u001b[0;32m    579\u001b[0m peercert \u001b[38;5;241m=\u001b[39m sslobj\u001b[38;5;241m.\u001b[39mgetpeercert()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\asyncio\\sslproto.py:559\u001b[0m, in \u001b[0;36mSSLProtocol._do_handshake\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 559\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLAgainErrors:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\ssl.py:979\u001b[0m, in \u001b[0;36mSSLObject.do_handshake\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Start the SSL/TLS handshake.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 979\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mdo_handshake()\n",
      "\u001b[1;31mSSLCertVerificationError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for 'www.cosmonetsolutions.com'. (_ssl.c:992)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mClientConnectorCertificateError\u001b[0m           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m urlh \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.cosmonetsolutions.com\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m aiohttp\u001b[38;5;241m.\u001b[39mClientSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m----> 6\u001b[0m      \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m session\u001b[38;5;241m.\u001b[39mget(urlh, allow_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msta\u001b[39m\u001b[38;5;124m'\u001b[39m,response\u001b[38;5;241m.\u001b[39mstatus)\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;66;03m# print('STATUS', response.status,type(response.text()),type(response.text),type(response))\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\aiohttp\\client.py:1194\u001b[0m, in \u001b[0;36m_BaseRequestContextManager.__aenter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1193\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _RetType:\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coro\n\u001b[0;32m   1195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resp\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\aiohttp\\client.py:578\u001b[0m, in \u001b[0;36mClientSession._request\u001b[1;34m(self, method, str_or_url, params, data, json, cookies, headers, skip_auto_headers, auth, allow_redirects, max_redirects, compress, chunked, expect100, raise_for_status, read_until_eof, proxy, proxy_auth, timeout, verify_ssl, fingerprint, ssl_context, ssl, server_hostname, proxy_headers, trace_request_ctx, read_bufsize, auto_decompress, max_line_size, max_field_size)\u001b[0m\n\u001b[0;32m    573\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m ceil_timeout(\n\u001b[0;32m    574\u001b[0m         real_timeout\u001b[38;5;241m.\u001b[39mconnect,\n\u001b[0;32m    575\u001b[0m         ceil_threshold\u001b[38;5;241m=\u001b[39mreal_timeout\u001b[38;5;241m.\u001b[39mceil_threshold,\n\u001b[0;32m    576\u001b[0m     ):\n\u001b[0;32m    577\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connector \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 578\u001b[0m         conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connector\u001b[38;5;241m.\u001b[39mconnect(\n\u001b[0;32m    579\u001b[0m             req, traces\u001b[38;5;241m=\u001b[39mtraces, timeout\u001b[38;5;241m=\u001b[39mreal_timeout\n\u001b[0;32m    580\u001b[0m         )\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mTimeoutError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ServerTimeoutError(\n\u001b[0;32m    583\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection timeout \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto host \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(url)\n\u001b[0;32m    584\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\aiohttp\\connector.py:544\u001b[0m, in \u001b[0;36mBaseConnector.connect\u001b[1;34m(self, req, traces, timeout)\u001b[0m\n\u001b[0;32m    541\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m trace\u001b[38;5;241m.\u001b[39msend_connection_create_start()\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 544\u001b[0m     proto \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection(req, traces, timeout)\n\u001b[0;32m    545\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closed:\n\u001b[0;32m    546\u001b[0m         proto\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\aiohttp\\connector.py:911\u001b[0m, in \u001b[0;36mTCPConnector._create_connection\u001b[1;34m(self, req, traces, timeout)\u001b[0m\n\u001b[0;32m    909\u001b[0m     _, proto \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_proxy_connection(req, traces, timeout)\n\u001b[0;32m    910\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 911\u001b[0m     _, proto \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_direct_connection(req, traces, timeout)\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m proto\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\aiohttp\\connector.py:1235\u001b[0m, in \u001b[0;36mTCPConnector._create_direct_connection\u001b[1;34m(self, req, traces, timeout, client_error)\u001b[0m\n\u001b[0;32m   1233\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1234\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m last_exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m last_exc\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\aiohttp\\connector.py:1204\u001b[0m, in \u001b[0;36mTCPConnector._create_direct_connection\u001b[1;34m(self, req, traces, timeout, client_error)\u001b[0m\n\u001b[0;32m   1197\u001b[0m server_hostname \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1198\u001b[0m     (req\u001b[38;5;241m.\u001b[39mserver_hostname \u001b[38;5;129;01mor\u001b[39;00m hinfo[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhostname\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1199\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sslcontext\n\u001b[0;32m   1200\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1201\u001b[0m )\n\u001b[0;32m   1203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1204\u001b[0m     transp, proto \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_create_connection(\n\u001b[0;32m   1205\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_factory,\n\u001b[0;32m   1206\u001b[0m         host,\n\u001b[0;32m   1207\u001b[0m         port,\n\u001b[0;32m   1208\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m   1209\u001b[0m         ssl\u001b[38;5;241m=\u001b[39msslcontext,\n\u001b[0;32m   1210\u001b[0m         family\u001b[38;5;241m=\u001b[39mhinfo[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfamily\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1211\u001b[0m         proto\u001b[38;5;241m=\u001b[39mhinfo[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproto\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1212\u001b[0m         flags\u001b[38;5;241m=\u001b[39mhinfo[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflags\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1213\u001b[0m         server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname,\n\u001b[0;32m   1214\u001b[0m         local_addr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_addr,\n\u001b[0;32m   1215\u001b[0m         req\u001b[38;5;241m=\u001b[39mreq,\n\u001b[0;32m   1216\u001b[0m         client_error\u001b[38;5;241m=\u001b[39mclient_error,\n\u001b[0;32m   1217\u001b[0m     )\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClientConnectorError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m   1219\u001b[0m     last_exc \u001b[38;5;241m=\u001b[39m exc\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\aiohttp\\connector.py:994\u001b[0m, in \u001b[0;36mTCPConnector._wrap_create_connection\u001b[1;34m(self, req, timeout, client_error, *args, **kwargs)\u001b[0m\n\u001b[0;32m    992\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop\u001b[38;5;241m.\u001b[39mcreate_connection(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    993\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m cert_errors \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m--> 994\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ClientConnectorCertificateError(req\u001b[38;5;241m.\u001b[39mconnection_key, exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[0;32m    995\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ssl_errors \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ClientConnectorSSLError(req\u001b[38;5;241m.\u001b[39mconnection_key, exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mClientConnectorCertificateError\u001b[0m: Cannot connect to host www.cosmonetsolutions.com:443 ssl:True [SSLCertVerificationError: (1, \"[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for 'www.cosmonetsolutions.com'. (_ssl.c:992)\")]"
     ]
    }
   ],
   "source": [
    "# urlh = 'https://www.cessco.ca/'\n",
    "# urlh = 'https://hellohero.com'\n",
    "urlh = 'https://www.cosmonetsolutions.com'\n",
    "\n",
    "async with aiohttp.ClientSession() as session:\n",
    "     async with session.get(urlh, allow_redirects=True, timeout=50) as response:\n",
    "        print('sta',response.status)\n",
    "\n",
    "        # print('STATUS', response.status,type(response.text()),type(response.text),type(response))\n",
    "        if response.status == 200:\n",
    "            # bs4_soup = BeautifulSoup(response.text(), 'html.parser')\n",
    "            # print('BS4 result',bs4_soup)\n",
    "            # response.text() coroutine for asynchroneity\n",
    "            response_text = await response.text()\n",
    "            print('r',type(response_text), response_text)\n",
    "            bs4_result = bs4_scrape(urlh, response_text)\n",
    "\n",
    "        try:\n",
    "            response = requests.get(urlh)\n",
    "            # res = bs4_scrape(urlh,response.text)\n",
    "            soup1 = BeautifulSoup(response.text, 'html.parser')\n",
    "            # soup2 = BeautifulSoup(await response.text(), 'html.parser')\n",
    "            bs5_result = bs4_scrape(urlh, response.text)\n",
    "            # print(res)\n",
    "        except Exception as e:\n",
    "            print('fail', e)\n",
    "\n",
    "print('res',bs4_result, bs5_result)\n",
    "\n",
    "# sel_scrape('http://www.academicresourcesolutions.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6a51bf3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [404]>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {'b':3,'c': 4}\n",
    "d = {'redirect':'asdf',**a}\n",
    "d\n",
    "requests.get('http://quickcarepharmacy.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4c58a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
